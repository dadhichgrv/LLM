# Iterative Workflow
from langgraph.graph import StateGraph, START, END
from langchain_core.messages import SystemMessage, HumanMessage, BaseMessage
from typing import Literal, Annotated, TypedDict
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver 
# Save state in RAM memory

class ChatState(TypedDict):
  messages : Annotated[list[BaseMessage],add_messages]

def chat_node(state:ChatState):
  messages = state["messages"]
  response = client_oai.invoke(messages)
  return {"messages":[response]}

checkpointer = MemorySaver()

graph = StateGraph(ChatState)

# Add nodes
graph.add_node("chat_node",chat_node)

# Add edges
graph.add_edge(START,"chat_node")
graph.add_edge("chat_node",END)

chatbot = graph.compile(checkpointer=checkpointer)
chatbot



thread_id = "1"

while True:
  user_input = input("Type your message")
  if user_input.strip().lower() in ["exit","quit","bye"]:
    break
 # Create config variable to pass thread_id
  config = {"configurable":{"thread_id":thread_id}}
  output_state = chatbot.invoke({"messages":[HumanMessage(content=user_input)]}, config=config)["messages"][-1].content
  print("AI: ",output_state)
